import os
from pathlib import Path
import math
import pandas as pd
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX
from llava.conversation import conv_templates, SeparatorStyle
from PIL import Image
import requests
import copy
import torch
import sys
import warnings


os.environ["HF_HOME"] = "/home/new_storage/sherlock/hf_cache"
os.environ["TRANSFORMERS_CACHE"] = "/home/new_storage/sherlock/hf_cache"
os.environ["HF_DATASETS_CACHE"] = "/home/new_storage/sherlock/hf_cache"
os.environ["HF_TOKENIZERS_CACHE"] = "/home/new_storage/sherlock/hf_cache"


# Load the OneVision model
pretrained = "lmms-lab/llava-onevision-qwen2-7b-ov-chat"
model_name = "llava_qwen"
device = "cuda"
device_map = "auto"
llava_model_args = {
    "multimodal": True,
    "attn_implementation": "sdpa",
}
tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map, **llava_model_args)  # Add any other thing you want to pass in llava_model_args

model.eval()


## function to process the image and text input

def process_sampled_images_from_dir(image_dir, text):
    # Get sorted list of image paths
    image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)])


    # Sample 13 evenly spaced images
    num_samples = min(13, len(image_files))
    indices = torch.linspace(0, len(image_files) - 1, steps=num_samples).long()
    sampled_paths = [image_files[i] for i in indices]

    results = {}

    for image_path in sampled_paths:
        print(f"Processing image: {image_path}")
        image = Image.open(image_path).convert("RGB")
        image_tensor = process_images([image], image_processor, model.config)
        image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]

        conv_template = "qwen_1_5"
        question = DEFAULT_IMAGE_TOKEN + f"\n{text}"
        conv = copy.deepcopy(conv_templates[conv_template])
        conv.append_message(conv.roles[0], question)
        conv.append_message(conv.roles[1], None)
        prompt_question = conv.get_prompt()

        input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt").unsqueeze(0).to(device)
        image_sizes = [image.size]

        cont = model.generate(
            input_ids,
            images=image_tensor,
            image_sizes=image_sizes,
            do_sample=False,
            temperature=0,
            max_new_tokens=220,
        )
        output_text = tokenizer.batch_decode(cont, skip_special_tokens=True)[0]
        results[image_path] = output_text

    return results


if __name__ == "__main__":
    directory_path = "/home/new_storage/sherlock/data/frames/TR0073"
    question = "This is a frame from a movie.Describe what the people in this image are doing. where their gaze is directed.Are they speaking. Is there Social interacion in this scene?"
    ## Describe what the people are doing. Focus on whether they are interacting.
    results = process_sampled_images_from_dir(directory_path, question)
    results_df = pd.DataFrame(results.items(), columns=['Image Path', 'Output Text'])
    results_df.to_excel('llava_results_TR0073.xlsx', index=False)
    print("Results:")
    for image_path, output_text in results.items():
        print(f"Image: {image_path}, Output: {output_text}")